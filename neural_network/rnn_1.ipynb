{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://karpathy.github.io/2015/05/21/rnn-effectiveness/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(32)\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams['figure.figsize'] = (10.0, 8.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# class RNN:\n",
    "#     def step(self, x):\n",
    "#         #update the hidden state\n",
    "#         self.h = np.tanh()\n",
    "\n",
    "data = open('input.txt', 'r').read()\n",
    "# print data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h [[ 0.01832831]\n",
      " [-0.21719961]\n",
      " [-0.20568907]\n",
      " [-0.24236387]\n",
      " [-0.2103193 ]\n",
      " [ 0.02150432]\n",
      " [ 0.20759876]\n",
      " [ 0.20007644]\n",
      " [ 0.20830389]\n",
      " [ 0.1856988 ]\n",
      " [ 0.00040154]\n",
      " [-0.00924468]\n",
      " [-0.20757687]\n",
      " [ 0.20603051]\n",
      " [ 0.01420357]\n",
      " [ 0.01270558]\n",
      " [ 0.00276013]\n",
      " [ 0.18224357]\n",
      " [-0.23414466]\n",
      " [ 0.24801997]\n",
      " [-0.23151675]\n",
      " [ 0.04903263]\n",
      " [-0.02593918]\n",
      " [-0.02388683]\n",
      " [ 0.03467857]\n",
      " [ 0.18871837]\n",
      " [ 0.00998784]\n",
      " [ 0.00917616]\n",
      " [-0.18681041]\n",
      " [ 0.18424572]\n",
      " [-0.18661908]\n",
      " [ 0.00981638]\n",
      " [-0.19455784]\n",
      " [ 0.24511228]\n",
      " [-0.22763786]\n",
      " [-0.22558603]\n",
      " [ 0.22301616]\n",
      " [-0.0029261 ]\n",
      " [-0.23983623]\n",
      " [-0.0176378 ]\n",
      " [ 0.21519047]\n",
      " [-0.18712575]\n",
      " [ 0.24918568]\n",
      " [-0.23525583]\n",
      " [-0.01829251]\n",
      " [ 0.00239445]\n",
      " [-0.01790335]\n",
      " [-0.20723261]\n",
      " [-0.01542303]\n",
      " [-0.00520361]\n",
      " [ 0.00319696]\n",
      " [-0.19667616]\n",
      " [ 0.00816098]\n",
      " [-0.02410593]\n",
      " [-0.19025361]\n",
      " [-0.24347447]\n",
      " [-0.00850076]\n",
      " [-0.22230596]\n",
      " [-0.00774856]\n",
      " [-0.03219417]\n",
      " [ 0.2097315 ]\n",
      " [ 0.02498915]\n",
      " [ 0.04847822]\n",
      " [ 0.18244793]\n",
      " [ 0.21764299]\n",
      " [ 0.18221728]\n",
      " [-0.00289502]\n",
      " [ 0.17757316]\n",
      " [ 0.20162152]\n",
      " [-0.03667393]\n",
      " [ 0.01877331]\n",
      " [ 0.23841486]\n",
      " [ 0.19682932]\n",
      " [ 0.20465336]\n",
      " [ 0.02744995]\n",
      " [ 0.06512529]\n",
      " [ 0.00827662]\n",
      " [ 0.02230657]\n",
      " [ 0.20249547]\n",
      " [ 0.23416202]\n",
      " [-0.20043705]\n",
      " [-0.01245137]\n",
      " [ 0.19320143]\n",
      " [ 0.22624529]\n",
      " [-0.18909294]\n",
      " [-0.00543212]\n",
      " [-0.02911097]\n",
      " [ 0.22583629]\n",
      " [ 0.18922549]\n",
      " [ 0.24492126]\n",
      " [ 0.02978413]\n",
      " [ 0.0420755 ]\n",
      " [ 0.05187454]\n",
      " [-0.18032983]\n",
      " [ 0.24159184]\n",
      " [ 0.18344652]\n",
      " [ 0.02357638]\n",
      " [-0.17137749]\n",
      " [ 0.22031794]\n",
      " [ 0.22563089]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# rand = np.random.RandomState(12)\n",
    "np.random.seed(32)\n",
    "\n",
    "hidden_size = 100\n",
    "seq_length = 25\n",
    "p = 0\n",
    "# data = \"hello\"\n",
    "data = open('input.txt', 'r').read()\n",
    "chars = list(set(data))\n",
    "vocab_size = len(chars)\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "inputs  = [char_to_ix[ch] for ch in data[p     : p + seq_length ]]\n",
    "targets = [char_to_ix[ch] for ch in data[p + 1 : p + seq_length + 1]]\n",
    "Wxh = np.random.randn(hidden_size, vocab_size)  * 0.01 # input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size) * 0.01 # hidden to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size)  * 0.01 # hidden to output\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "by = np.zeros((vocab_size , 1)) # output bias\n",
    "hprev = np.zeros((hidden_size,1))\n",
    "\n",
    "learning_rate = 1e-1\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by)\n",
    "\n",
    "def lossFun(inputs, targets, hprev):\n",
    "# def lossFun(inputs, hprev):\n",
    "    \"\"\"\n",
    "    inputs, targes are both list of integers.\n",
    "    hprev is Hx1 array of initial state\n",
    "    returns the loss, gradients on model parameters, and last hidden state\n",
    "    \"\"\"\n",
    "    xs, hs, ys, ps = {}, {}, {}, {}\n",
    "#     hs[-1] = np.copy(hprev)\n",
    "    loss = 0\n",
    "    hs[-1] = np.copy(hprev)\n",
    "    \n",
    "    # forward pass\n",
    "    for t in xrange(len(inputs)):\n",
    "        xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
    "        xs[t][inputs[t]] = 1\n",
    "#         print 'xs[t]', xs[t]\n",
    "        dot_x = np.dot(Wxh, xs[t])\n",
    "        dot_h_prev = np.dot(Whh, hs[t-1])\n",
    "\n",
    "#         print 'dot_x', dot_x\n",
    "#         print 'dot_h_prev', dot_h_prev\n",
    "\n",
    "        \n",
    "        hs[t] = np.tanh(dot_x + dot_h_prev + bh) # hidden state\n",
    "#         print 'hs[t]', hs[t]\n",
    "        ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilites for next chars\n",
    "#         print 'ys[i]', ys[t]\n",
    "        ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
    "        l = -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
    "#         print 'l', l\n",
    "        loss += l\n",
    "#     print 'loss', loss\n",
    "    # backward pass: compute gradients going backwards\n",
    "    dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "    dhnext = np.zeros_like(hs[0])\n",
    "#     print 'ps', ps\n",
    "    for t in reversed(xrange(len(inputs))):\n",
    "        dy = np.copy(ps[t])\n",
    "#         print 'dy', dy\n",
    "        dy[targets[t]] -= 1  # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "#         print 'dy', dy\n",
    "        dWhy += np.dot(dy, hs[t].T)\n",
    "#         print 'dWhy', dWhy\n",
    "        dby += dy\n",
    "#         print 'dby', dby\n",
    "        dh = np.dot(Why.T, dy) + dhnext # backprop into h\n",
    "#         print 'dh', dh\n",
    "    \n",
    "#         print 'dh', dh\n",
    "#         print 'hs[t][0]', hs[t][0]\n",
    "#         print 'dh[0]', dh[0]\n",
    "#         print 'hs[t]', hs[t]\n",
    "        dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity\n",
    "#         print 'dhraw[0]', dhraw[0]\n",
    "        dbh += dhraw\n",
    "#         print 'dbh', dbh\n",
    "    \n",
    "#         print 'dbh', dbh\n",
    "    \n",
    "        dWxh += np.dot(dhraw, xs[t].T)\n",
    "        dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "        dhnext = np.dot(Whh.T, dhraw)\n",
    "#         print 'dhnext', dhnext\n",
    "\n",
    "#         print 'dh', dh\n",
    "#         print 'dbh', dbh\n",
    "#         print 'dWxh', dWxh\n",
    "#         print 'dWhy', dWhy\n",
    "#         print 'dWhh', dWhh\n",
    "#         print 'dby', dby\n",
    "\n",
    "    for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "        np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "    return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]\n",
    "\n",
    "def sample(h, seed_ix, n):\n",
    "    \"\"\"\n",
    "    sample a sequence of integers from the model\n",
    "    h is memory state, seed_ix is seed letter for first time step\n",
    "    \"\"\"\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[seed_ix] = 1\n",
    "    ixes = []\n",
    "#     print 'inputs', h, seed_ix, n\n",
    "    for t in xrange(n):\n",
    "        h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "        y = np.dot(Why, h) + by\n",
    "        if t == 0:\n",
    "#             print 'wxh', Wxh\n",
    "#             print 'x', x\n",
    "#             print 'whh', Whh\n",
    "#             print 'intputs[0]', seed_ix\n",
    "            print 'h', h\n",
    "#             print 'bh', bh\n",
    "#             print 'h1', Wxh,\n",
    "#             print 'Wxh', Wxh\n",
    "#             print 'h2', Whh, h\n",
    "#             print 'h3', bh#             print 'y', y\n",
    "        p = np.exp(y) / np.sum(np.exp(y))\n",
    "#         if t == 0:\n",
    "#             print 'p', p\n",
    "        ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[ix] = 1\n",
    "        ixes.append(ix)\n",
    "    return ixes\n",
    "\n",
    "\n",
    "losses = []\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "for i in range(5):\n",
    "\n",
    "    if p+seq_length+1 >= len(data) or i == 0: \n",
    "        hprev = np.zeros((hidden_size,1)) # reset RNN memory\n",
    "        p = 0 # go from start of data\n",
    "    inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "    targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "#     print 'inputs', inputs\n",
    "#     print 'targets', targets\n",
    "#     print 'wxh', Wxh\n",
    "#     print 'whh', Whh\n",
    "#     print 'why', Why\n",
    "#     print 'bh', bh\n",
    "#     print 'by', by\n",
    "#     print 'h_prev', hprev\n",
    "\n",
    "    loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "#     print 'hprev', hprev\n",
    "#     print 'dwxh', dWxh\n",
    "#     print 'dwhh', dWhh\n",
    "#     print 'dwhy', dWhy\n",
    "#     print 'dbh', dbh\n",
    "#     print 'dby', dby\n",
    "    \n",
    "#     print 'dWxh', dWxh\n",
    "    \n",
    "    p += seq_length # move data pointer\n",
    "\n",
    "    mWxh += dWxh * dWxh\n",
    "    mWhh += dWhh * dWhh\n",
    "    mWhy += dWhy * dWhy\n",
    "    mbh += dbh * dbh\n",
    "    mby += dby * dby\n",
    "    \n",
    "\n",
    "    Wxh += -learning_rate * dWxh / np.sqrt(mWxh + 1e-8) # adagrad update\n",
    "    Whh += -learning_rate * dWhh / np.sqrt(mWhh + 1e-8)\n",
    "    Why += -learning_rate * dWhy / np.sqrt(mWhy + 1e-8)\n",
    "    bh += -learning_rate * dbh / np.sqrt(mbh + 1e-8)\n",
    "    by += -learning_rate * dby / np.sqrt(mby + 1e-8)\n",
    "\n",
    "#     print 'wxh', Wxh\n",
    "#     print 'whh', Whh\n",
    "#     print 'why', Why\n",
    "#     print 'bh',  bh\n",
    "#     print 'by',  by\n",
    "    \n",
    "#     print 'Wxh', Wxh\n",
    "\n",
    "    if i % 100 == 0:\n",
    "#         print inputs\n",
    "        sample_ix = sample(hprev, inputs[0], 200)\n",
    "#         txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "#         print '-----\\n %s \\n------' % (txt,)\n",
    "#         print 'loss', loss\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "#     if i % 100 == 0: print 'iter %d, loss: %f' % (i, smooth_loss) # print progress\n",
    "    losses.append(loss)\n",
    "\n",
    "#     print loss\n",
    "\n",
    "# plt.plot(losses)\n",
    "# print losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31, 81, 65, 15, 40, 0, 31, 40, 15, 15, 40, 13, 83, 44, 9, 82, 0, 81, 66, 66, 82, 64, 17, 86, 65]\n",
      "[81, 65, 15, 40, 0, 31, 40, 15, 15, 40, 13, 83, 44, 9, 82, 0, 81, 66, 66, 82, 64, 17, 86, 65, 82]\n",
      "[[-0.01287879  0.          0.         ...,  0.          0.         -0.01581844]\n",
      " [-0.00474426  0.          0.         ...,  0.          0.          0.00425313]\n",
      " [ 0.0178115   0.          0.         ...,  0.          0.          0.00580286]\n",
      " ..., \n",
      " [ 0.00044909  0.          0.         ...,  0.          0.          0.00384821]\n",
      " [-0.01153658  0.          0.         ...,  0.          0.         -0.003235  ]\n",
      " [-0.00535685  0.          0.         ...,  0.          0.          0.00869725]]\n"
     ]
    }
   ],
   "source": [
    "p = 0\n",
    "inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "hprev = np.zeros((hidden_size,1))\n",
    "print inputs\n",
    "print targets\n",
    "# print hprev\n",
    "loss, a, _, _, _, _, _ = lossFun(inputs, targets, hprev)\n",
    "# print loss\n",
    "print a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
