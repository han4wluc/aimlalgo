{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import struct\n",
    "import numpy as np\n",
    "import array\n",
    "import time\n",
    "import scipy.sparse\n",
    "import scipy.optimize\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" The Softmax Regression class \"\"\"\n",
    "\n",
    "class SoftmaxRegression(object):\n",
    "    \n",
    "    \"\"\" Initialize parameters of the Regressor object \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, num_classes, lam):\n",
    "        \n",
    "        \"\"\" Initialize parameters of the Regressor object \"\"\"\n",
    "\n",
    "        self.input_size = input_size   # input vector size\n",
    "        self.num_classes = num_classes  # number of classes\n",
    "        self.lam = lam                  # weight decay parameter\n",
    "        \n",
    "        \"\"\" Randomly initialize the class weights \"\"\"\n",
    "        \n",
    "        rand = np.random.RandomState(int(time.time()))\n",
    "\n",
    "        rand = np.random.RandomState(10)\n",
    "        self.theta = 0.0005 * rand.rand(num_classes, input_size)\n",
    "        \n",
    "#         self.theta = 0.005 * np.asarray(rand.normal(size = (num_classes * input_size, 1)))\n",
    "        \n",
    "    def get_ground_truth(self, labels):\n",
    "        \"\"\" Returns the groundtruth matrix for a set of labels\"\"\"\n",
    "\n",
    "        \"\"\" Prepare data needed to construct ground truth matrix \"\"\"\n",
    "        labels = np.array(labels).flatten()\n",
    "        data = np.ones(len(labels))\n",
    "        indptr = np.arange(len(labels)+1)\n",
    "        \n",
    "        \"\"\" Compute the groundtruth matrix and return \"\"\"\n",
    "        \n",
    "        ground_truth = scipy.sparse.csr_matrix((data, labels, indptr))\n",
    "        ground_truth = np.transpose(ground_truth.todense())\n",
    "        \n",
    "        return ground_truth\n",
    "    \n",
    "    def softmax_cost(self, theta, input, labels):\n",
    "        \"\"\" Returns the cost and gradient of 'theta' at a particular 'theta' \"\"\"\n",
    "        \n",
    "        \"\"\" Compute the groundtruth matrix \"\"\"\n",
    "        ground_truth = self.get_ground_truth(labels)\n",
    "        \n",
    "        \"\"\" Reshape 'theta' for ease of computation \"\"\"\n",
    "        \n",
    "        theta = theta.reshape(self.num_classes, self.input_size)\n",
    "        \n",
    "        \"\"\" Compute the class probabilities for each example \"\"\"\n",
    "        \n",
    "        print 'X', input\n",
    "        print 'X.T[0]', input.T[0]\n",
    "        print 'X.shape', input.shape\n",
    "        print 'X.sum()', input.sum()\n",
    "        print 'theta', theta\n",
    "        print 'theta.shape', theta.shape\n",
    "        print 'theta.sum()', theta.sum()\n",
    "        \n",
    "        theta_x = np.dot(theta, input)\n",
    "        print 'theta_x', theta_x\n",
    "        print 'theta_x.sum()', theta_x.sum()\n",
    "        print 'theta_x.shape', theta_x.shape\n",
    "        hypothesis = np.exp(theta_x)\n",
    "        print 'h', hypothesis\n",
    "        print 'summation', np.sum(hypothesis, axis = 0)\n",
    "        probabilities = hypothesis / np.sum(hypothesis, axis = 0)\n",
    "        print 'probabilities', probabilities\n",
    "        \"\"\" Compute the traditional cost term \"\"\"\n",
    "        \n",
    "        cost_examples = np.multiply(ground_truth, np.log(probabilities))\n",
    "        traditional_cost = -(np.sum(cost_examples) / input.shape[1])\n",
    "        \n",
    "        \"\"\" Compute the weight decay term \"\"\"\n",
    "        \n",
    "        theta_squared = np.multiply(theta, theta)\n",
    "        weight_decay = 0.5 * self.lam * np.sum(theta_squared)\n",
    "        \n",
    "        \"\"\" Add both terms to get the cost \"\"\"\n",
    "        cost = traditional_cost + weight_decay\n",
    "        \n",
    "        \"\"\" Compute the unroll 'theta' gradient \"\"\"\n",
    "        \n",
    "        theta_grad = -np.dot(ground_truth - probabilities, np.transpose(input))\n",
    "        theta_grad = theta_grad / input.shape[1] + self.lam * theta\n",
    "        theta_grad = np.array(theta_grad)\n",
    "        theta_grad = theta_grad.flatten()\n",
    "        \n",
    "        return [cost, theta_grad]\n",
    "    \n",
    "    def softmax_predict(self, theta, input):\n",
    "        \n",
    "        \"\"\" Returns predicted classes for a set of inputs \"\"\"\n",
    "        \n",
    "        \"\"\" Reshape 'theta' for ease of computation \"\"\"\n",
    "        \n",
    "        theta = theta.reshape(self.num_classes, self.input_size)\n",
    "        \n",
    "        \"\"\" Compute the class probabilities for each example \"\"\"\n",
    "        \n",
    "        theta_x = np.dot(theta, input)\n",
    "        hypothesis = np.exp(theta_x)\n",
    "        probabilities = hypothesis / np.sum(hypothesis, axis = 0)\n",
    "        \n",
    "        \"\"\" Give the predictions based on probability values \"\"\"\n",
    "        \n",
    "        predictions = np.zeros((input.shape[1], 1))\n",
    "        predictions[:,0] = np.argmax(probabilities, axis=0)\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "def load_mnist_images(file_name):\n",
    "\n",
    "    \"\"\" Open the file \"\"\"\n",
    "\n",
    "    image_file = open(file_name, 'rb')\n",
    "\n",
    "    \"\"\" Read header information from the file \"\"\"\n",
    "\n",
    "    head1 = image_file.read(4)\n",
    "    head2 = image_file.read(4)\n",
    "    head3 = image_file.read(4)\n",
    "    head4 = image_file.read(4)\n",
    "\n",
    "    \"\"\" Format the header information for useful data \"\"\"\n",
    "\n",
    "    num_examples = struct.unpack('>I', head2)[0]\n",
    "    num_rows     = struct.unpack('>I', head3)[0]\n",
    "    num_cols     = struct.unpack('>I', head4)[0]\n",
    "    \n",
    "\n",
    "    \"\"\" Initialize dataset as array of zeros \"\"\"\n",
    "\n",
    "    dataset = np.zeros((num_rows * num_cols, num_examples))\n",
    "\n",
    "    \"\"\" Read the actual image data \"\"\"\n",
    "\n",
    "    images_raw = array.array('B', image_file.read())\n",
    "    image_file.close()\n",
    "\n",
    "    \"\"\" Arrange the data in columns \"\"\"\n",
    "\n",
    "    for i in range(num_examples):\n",
    "\n",
    "        limit1 = num_rows * num_cols * i\n",
    "        limit2 = num_rows * num_cols * (i + 1)\n",
    "\n",
    "        dataset[:, i] = images_raw[limit1 : limit2]\n",
    "\n",
    "    \"\"\" Normalize and return the dataset \"\"\"\n",
    "\n",
    "    return dataset / 255\n",
    "\n",
    "def load_mnist_labels(file_name):\n",
    "\n",
    "    \"\"\" Open the file \"\"\"\n",
    "\n",
    "    label_file = open(file_name, 'rb')\n",
    "\n",
    "    \"\"\" Read header information from the file \"\"\"\n",
    "\n",
    "    head1 = label_file.read(4)\n",
    "    head2 = label_file.read(4)\n",
    "\n",
    "    \"\"\" Format the header information for useful data \"\"\"\n",
    "    \n",
    "    num_examples = struct.unpack('>I', head2)[0]\n",
    "    \n",
    "    \"\"\" Initialize data labels as array of zeros \"\"\"\n",
    "\n",
    "    labels = np.zeros((num_examples, 1), dtype = np.int)\n",
    "\n",
    "    \"\"\" Read the label data \"\"\"\n",
    "\n",
    "    labels_raw = array.array('b', label_file.read())\n",
    "    label_file.close()\n",
    "\n",
    "    \"\"\" Copy and return the label data \"\"\"\n",
    "\n",
    "    labels[:, 0] = labels_raw[:]\n",
    "\n",
    "    return labels\n",
    "        \n",
    "def execute_softmax_regression():\n",
    "\n",
    "    \"\"\" Initialize parameters of the Regressor \"\"\"\n",
    "\n",
    "    input_size = 784       # input vector size\n",
    "    num_classes = 10       # number of classes\n",
    "    lam = 0.0001           # weight decay parameter\n",
    "    max_iterations = 100   # number of optimization iterations\n",
    "\n",
    "    \"\"\" Load MNIST training images and labels \"\"\"\n",
    "\n",
    "    training_data = load_mnist_images('./train-images-idx3-ubyte')\n",
    "    training_labels = load_mnist_labels('./train-labels-idx1-ubyte')\n",
    "\n",
    "    \"\"\" Initialize Softmax Regressor with the above parameters \"\"\"\n",
    "\n",
    "    regressor = SoftmaxRegression(input_size, num_classes, lam)\n",
    "    \n",
    "    \"\"\" Run the L-BFGS algorithm to get the optimal parameter values \"\"\"\n",
    "    \n",
    "    opt_solution = scipy.optimize.minimize(\n",
    "        regressor.softmax_cost,\n",
    "        regressor.theta,\n",
    "        args = (\n",
    "            training_data,\n",
    "            training_labels,\n",
    "        ),\n",
    "        method = 'L-BFGS-B',\n",
    "        jac = True,\n",
    "        options = {\n",
    "            'maxiter': max_iterations\n",
    "        }\n",
    "    )\n",
    "    opt_theta = opt_solution.x\n",
    "\n",
    "    \"\"\" Load MNIST test images and labels \"\"\"\n",
    "\n",
    "    test_data = load_mnist_images('./t10k-images-idx3-ubyte')\n",
    "    test_labels = load_mnist_labels('./t10k-labels-idx1-ubyte')\n",
    "\n",
    "    \"\"\" Obtain predictions from the trained model \"\"\"\n",
    "\n",
    "    predictions = regressor.softmax_predict(opt_theta, test_data)\n",
    "\n",
    "    \"\"\" Print accuracy of the trained model \"\"\"\n",
    "\n",
    "    correct = test_labels[:, 0] == predictions[:, 0]\n",
    "    print \"\"\"Accuray :\"\"\", np.mean(correct)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "global name 'n_of_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-ed1809afc70f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\"\"\" Initialize Softmax Regressor with the above parameters \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mregressor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSoftmaxRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\"\"\" Run the L-BFGS algorithm to get the optimal parameter values \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-c6595a4f0047>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_size, num_classes, lam)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mrand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtheta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0005\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mrand\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_of_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m#         self.theta = 0.005 * np.asarray(rand.normal(size = (num_classes * input_size, 1)))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'n_of_features' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\" Initialize parameters of the Regressor \"\"\"\n",
    "\n",
    "input_size = 784       # input vector size\n",
    "num_classes = 10       # number of classes\n",
    "lam = 0.0001           # weight decay parameter\n",
    "max_iterations = 1   # number of optimization iterations\n",
    "\n",
    "\"\"\" Load MNIST training images and labels \"\"\"\n",
    "\n",
    "training_data = load_mnist_images('./train-images-idx3-ubyte')[:,:1000]\n",
    "training_labels = load_mnist_labels('./train-labels-idx1-ubyte')[:,:1000]\n",
    "\n",
    "\"\"\" Initialize Softmax Regressor with the above parameters \"\"\"\n",
    "\n",
    "regressor = SoftmaxRegression(input_size, num_classes, lam)\n",
    "\n",
    "\"\"\" Run the L-BFGS algorithm to get the optimal parameter values \"\"\"\n",
    "\n",
    "opt_solution = scipy.optimize.minimize(\n",
    "    regressor.softmax_cost,\n",
    "    regressor.theta,\n",
    "    args = (\n",
    "        training_data,\n",
    "        training_labels,\n",
    "    ),\n",
    "    method = 'L-BFGS-B',\n",
    "    jac = True,\n",
    "    options = {\n",
    "        'maxiter': max_iterations\n",
    "    }\n",
    ")\n",
    "# opt_theta = opt_solution.x\n",
    "\n",
    "# \"\"\" Load MNIST test images and labels \"\"\"\n",
    "\n",
    "# test_data = load_mnist_images('./t10k-images-idx3-ubyte')\n",
    "# test_labels = load_mnist_labels('./t10k-labels-idx1-ubyte')\n",
    "\n",
    "# \"\"\" Obtain predictions from the trained model \"\"\"\n",
    "\n",
    "# predictions = regressor.softmax_predict(opt_theta, test_data)\n",
    "\n",
    "# \"\"\" Print accuracy of the trained model \"\"\"\n",
    "\n",
    "# correct = test_labels[:, 0] == predictions[:, 0]\n",
    "# print \"\"\"Accuray :\"\"\", np.mean(correct)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 0 4 ..., 5 6 8]\n",
      "(60000, 10)\n"
     ]
    }
   ],
   "source": [
    "# def get_ground_truth(self, labels):\n",
    "#     \"\"\" Returns the groundtruth matrix for a set of labels\"\"\"\n",
    "\n",
    "#     \"\"\" Prepare data needed to construct ground truth matrix \"\"\"\n",
    "#     labels = np.array(labels).flatten()\n",
    "#     data = np.ones(len(labels))\n",
    "#     indptr = np.arange(len(labels)+1)\n",
    "\n",
    "#     \"\"\" Compute the groundtruth matrix and return \"\"\"\n",
    "\n",
    "#     ground_truth = scipy.sparse.csr_matrix((data, labels, indptr))\n",
    "#     ground_truth = np.transpose(ground_truth.todense())\n",
    "\n",
    "#     return ground_truth\n",
    "\n",
    "# training_labels\n",
    "\n",
    "print labels\n",
    "labels = np.array(training_labels).flatten()\n",
    "data = np.ones(len(labels))\n",
    "indptr = np.arange(len(labels)+1)\n",
    "# print indptr\n",
    "\n",
    "ground_truth = scipy.sparse.csr_matrix((data, labels, indptr))\n",
    "# print ground_truth.todense()\n",
    "ground_truth = np.transpose(ground_truth.todense()).T\n",
    "# print ground_truth[0]\n",
    "print ground_truth.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 10)\n"
     ]
    }
   ],
   "source": [
    "def to_one_hot(y):\n",
    "    m = y.shape[0]\n",
    "    \n",
    "    K = len(np.unique(y))\n",
    "    one_hot = np.zeros(shape=(m, K))\n",
    "    for i, row in enumerate(one_hot):\n",
    "        if i < 10:\n",
    "            idx = y[i]\n",
    "            row[idx] = 1\n",
    "    return one_hot\n",
    "# print to_one_hot(labels)[0]\n",
    "one_hot_labels = to_one_hot(labels)\n",
    "print one_hot_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Initialize Softmax Regressor with the above parameters '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "input_size = 784       # input vector size\n",
    "num_classes = 10       # number of classes\n",
    "lam = 0.0001           # weight decay parameter\n",
    "max_iterations = 100   # number of optimization iterations\n",
    "\n",
    "\"\"\" Load MNIST training images and labels \"\"\"\n",
    "\n",
    "training_data = load_mnist_images('./train-images-idx3-ubyte')\n",
    "training_labels = load_mnist_labels('./train-labels-idx1-ubyte')\n",
    "\n",
    "\"\"\" Initialize Softmax Regressor with the above parameters \"\"\"\n",
    "\n",
    "# regressor = SoftmaxRegression(input_size, num_classes, lam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "regressor.softmax_cost(self, theta, training_data, training_labels):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 785)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-b0502feefd2a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmatrix_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatrix_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mm_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test' is not defined"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./mnist_train.csv')[:1000]\n",
    "# test = pd.read_csv('./mnist_test.csv', header=None)\n",
    "\n",
    "matrix = df.as_matrix()\n",
    "print matrix.shape\n",
    "m = matrix.shape[0]\n",
    "y = matrix[:,0:1]\n",
    "X = matrix[:,1:]\n",
    "\n",
    "# matrix_test = test.as_matrix()\n",
    "# X_test = matrix_test[:,1:]\n",
    "# m_test = X_test.shape[0]\n",
    "# y_test = matrix_test[:,0:1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_of_features = X.shape[1]\n",
    "k = len(np.unique(y))\n",
    "rand = np.random.RandomState(10)\n",
    "theta = 0.0005 * rand.rand(n_of_features, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X [[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "X.T[0] [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0  51 159 253 159  50   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0  48 238 252 252 252 237   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0  54 227 253 252 239 233 252  57   6   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0  10  60 224 252 253 252 202  84 252\n",
      " 253 122   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0 163 252 252 252 253 252 252  96 189 253 167   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0  51 238 253 253 190 114 253 228\n",
      "  47  79 255 168   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0  48 238 252 252 179  12  75 121  21   0   0 253 243  50   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0  38 165 253 233 208  84   0   0\n",
      "   0   0   0   0 253 252 165   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   7 178 252 240  71  19  28   0   0   0   0   0   0 253 252 195   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0  57 252 252  63   0   0   0\n",
      "   0   0   0   0   0   0 253 252 195   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0 198 253 190   0   0   0   0   0   0   0   0   0   0 255 253\n",
      " 196   0   0   0   0   0   0   0   0   0   0   0  76 246 252 112   0   0\n",
      "   0   0   0   0   0   0   0   0 253 252 148   0   0   0   0   0   0   0\n",
      "   0   0   0   0  85 252 230  25   0   0   0   0   0   0   0   0   7 135\n",
      " 253 186  12   0   0   0   0   0   0   0   0   0   0   0  85 252 223   0\n",
      "   0   0   0   0   0   0   0   7 131 252 225  71   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0  85 252 145   0   0   0   0   0   0   0  48 165\n",
      " 252 173   0   0   0   0   0   0   0   0   0   0   0   0   0   0  86 253\n",
      " 225   0   0   0   0   0   0 114 238 253 162   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0  85 252 249 146  48  29  85 178 225 253\n",
      " 223 167  56   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "  85 252 252 252 229 215 252 252 252 196 130   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0  28 199 252 252 253 252 252 233\n",
      " 145   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0  25 128 252 253 252 141  37   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0]\n",
      "X.shape (784, 1000)\n",
      "X.sum() 25634175\n",
      "theta [[  3.85660322e-04   1.03759747e-05   3.16824117e-04 ...,   4.66530193e-04\n",
      "    2.79538175e-04   3.08552539e-04]\n",
      " [  2.34494602e-04   4.97414883e-04   4.66736855e-04 ...,   3.08559443e-04\n",
      "    2.61600180e-04   1.31760154e-04]\n",
      " [  2.66439515e-04   1.67756526e-04   1.53673573e-04 ...,   5.97942741e-05\n",
      "    1.59763830e-04   2.88624047e-04]\n",
      " ..., \n",
      " [  4.30791245e-04   3.23250704e-04   1.18635170e-04 ...,   4.50958641e-04\n",
      "    2.83464425e-04   1.39533453e-04]\n",
      " [  2.12040461e-04   6.69079080e-05   3.39008463e-04 ...,   4.56860688e-04\n",
      "    3.74694615e-04   2.45380228e-04]\n",
      " [  3.84250382e-04   1.95418337e-05   1.75991613e-04 ...,   4.98374494e-04\n",
      "    1.15055453e-04   4.47175715e-04]]\n",
      "theta.shape (10, 784)\n",
      "theta.sum() 1.91790253931\n",
      "theta_x [[ 7.08339261  5.23849615  4.13558312 ...,  5.82939387  6.8328044\n",
      "   6.00333485]\n",
      " [ 8.53017741  5.3001202   4.3652769  ...,  5.99706937  6.55353225\n",
      "   6.14876512]\n",
      " [ 7.25225128  4.80172892  4.64594513 ...,  5.31732967  6.15518455\n",
      "   5.07702501]\n",
      " ..., \n",
      " [ 7.97986899  5.0271919   4.9095439  ...,  5.55547127  6.04162307\n",
      "   6.57881962]\n",
      " [ 7.58998985  4.71076782  4.00131832 ...,  5.33989697  6.54660479\n",
      "   5.99652437]\n",
      " [ 7.79769889  5.25516703  4.38496978 ...,  6.16813784  6.6561298\n",
      "   6.79176922]]\n",
      "theta_x.sum() 63656.9045151\n",
      "theta_x.shape (10, 1000)\n",
      "h [[ 1192.00567913   188.38658363    62.52604073 ...,   340.15244128\n",
      "    927.78906354   404.77641231]\n",
      " [ 5065.34441846   200.36089215    78.67118128 ...,   402.2482248\n",
      "    701.71844316   468.1389345 ]\n",
      " [ 1411.27845732   121.72068055   104.16176589 ...,   203.83883824\n",
      "    471.15378552   160.29646623]\n",
      " ..., \n",
      " [ 2921.54828516   152.50416334   135.5775637  ...,   258.64882971\n",
      "    420.57510574   719.68931977]\n",
      " [ 1978.29343871   111.13746087    54.67017514 ...,   208.49122831\n",
      "    696.87411847   402.02905858]\n",
      " [ 2434.99235085   191.55347754    80.23579883 ...,   477.29647669\n",
      "    777.535889     890.48763759]]\n",
      "summation [  2.39572004e+04   1.32672912e+03   7.60332530e+02   3.38968520e+03\n",
      "   1.74908705e+04   8.29205080e+02   7.72008040e+04   1.61359845e+02\n",
      "   2.90877098e+03   1.20269464e+04   3.84971875e+02   7.26273677e+04\n",
      "   1.28417954e+04   1.99050210e+02   5.51731239e+03   5.11167415e+03\n",
      "   1.05302738e+04   3.03272102e+02   8.99810926e+02   5.48426815e+04\n",
      "   6.31301615e+04   5.46467948e+02   6.64463157e+02   2.44586074e+03\n",
      "   2.61418498e+05   2.84593843e+02   9.69835880e+05   4.05906244e+05\n",
      "   5.73588963e+02   6.83831937e+03   3.68836847e+04   7.61207184e+02\n",
      "   2.21362345e+03   1.06932605e+05   5.78768158e+02   3.10711529e+04\n",
      "   1.37596151e+05   2.47456770e+03   8.30592241e+03   3.61961797e+02\n",
      "   1.23826943e+04   2.36530015e+02   1.02199731e+03   8.08848391e+02\n",
      "   4.24858139e+03   3.27332678e+03   2.64236883e+03   7.07543921e+02\n",
      "   3.61979952e+04   1.08498080e+03   7.45057691e+05   1.21678514e+04\n",
      "   6.74515475e+02   3.44428553e+03   3.59062457e+04   3.25835405e+05\n",
      "   1.57843234e+03   4.52326042e+04   6.62373104e+02   1.62964799e+04\n",
      "   1.01471294e+03   2.08848017e+04   9.43190742e+05   6.75288465e+03\n",
      "   6.78491318e+02   9.92672279e+03   4.89828772e+02   4.16940680e+03\n",
      "   4.47109687e+05   2.57820881e+03   1.88809148e+03   1.58786171e+02\n",
      "   7.77862687e+03   6.94966300e+03   5.04237414e+04   7.70959706e+03\n",
      "   7.20362535e+02   1.53842927e+03   3.63101154e+03   7.27450010e+03\n",
      "   1.13418030e+05   2.74793205e+05   8.41017578e+03   1.09168467e+03\n",
      "   6.44619124e+03   2.28162063e+03   1.09472942e+04   4.45321030e+04\n",
      "   5.29950171e+03   1.65406532e+04   6.89225374e+03   4.64463705e+02\n",
      "   1.67603314e+04   1.04345106e+04   1.29707030e+05   9.58579890e+02\n",
      "   1.64772657e+04   2.18343799e+03   2.57877600e+02   9.29690509e+02\n",
      "   3.39201002e+03   1.85456574e+02   1.22986381e+03   1.85006468e+02\n",
      "   1.28334241e+03   7.33501982e+03   2.67785768e+04   1.20113346e+04\n",
      "   2.02143448e+03   5.04145922e+03   1.01789652e+06   2.13326783e+02\n",
      "   1.80889054e+03   1.95237344e+05   2.01742599e+03   6.25468318e+03\n",
      "   2.12530243e+05   5.01172713e+03   4.36445237e+05   2.11367153e+05\n",
      "   7.41305481e+05   4.02431057e+03   4.70490473e+03   1.13615313e+02\n",
      "   7.66362993e+03   9.43486828e+03   3.74950230e+03   1.59440783e+03\n",
      "   2.82634581e+04   2.71644010e+03   1.02790298e+04   8.91072169e+02\n",
      "   1.02922583e+03   4.16545813e+03   4.05120163e+04   1.69168253e+03\n",
      "   1.01255425e+04   1.09445705e+04   4.58918893e+03   3.15436984e+02\n",
      "   2.95093589e+03   9.89531554e+02   5.17828957e+03   1.17282579e+03\n",
      "   1.22907734e+03   1.47319058e+04   1.31410001e+04   8.87245980e+02\n",
      "   6.40945750e+04   7.71067640e+03   8.04962543e+03   4.17690758e+02\n",
      "   5.97230060e+02   2.26655164e+03   1.69210821e+04   7.19697986e+03\n",
      "   3.14967916e+04   4.87835832e+02   1.86886720e+04   2.03182129e+03\n",
      "   8.97481648e+04   2.16984863e+03   4.22047415e+03   9.94295947e+04\n",
      "   2.83064135e+04   1.96035586e+03   5.27173199e+03   2.49192092e+04\n",
      "   1.25214885e+05   2.17240123e+03   5.40209415e+04   4.94973051e+03\n",
      "   4.84394855e+02   2.14383496e+02   2.94414902e+03   1.41579613e+03\n",
      "   2.13394530e+03   3.10581301e+03   9.15271517e+04   5.83886658e+02\n",
      "   6.02780704e+04   8.10776877e+03   4.24875232e+03   3.20333450e+02\n",
      "   1.45354674e+04   3.39371820e+03   1.90142173e+05   1.20550831e+04\n",
      "   3.48315854e+04   1.34471940e+04   6.83071996e+02   2.72753264e+04\n",
      "   8.04481035e+03   1.47772155e+04   2.78685147e+03   1.03582286e+04\n",
      "   4.69567722e+03   1.22060883e+04   8.23409741e+03   8.52611087e+02\n",
      "   1.06434052e+03   8.04154320e+03   2.90546821e+04   3.71042925e+03\n",
      "   9.58970152e+02   7.59525339e+04   1.49593447e+04   1.00818039e+03\n",
      "   1.32477830e+05   6.57160336e+02   1.11218411e+03   5.55751239e+02\n",
      "   1.49177267e+05   1.07171630e+04   4.16107277e+04   9.58781570e+03\n",
      "   1.58484147e+03   6.26045438e+03   7.41002470e+03   9.16005880e+04\n",
      "   7.18053098e+04   1.58501715e+05   6.42749881e+03   6.14656357e+03\n",
      "   4.71222063e+04   2.28308956e+03   2.58852677e+03   9.86590407e+02\n",
      "   9.82634624e+04   1.11515360e+03   7.09249348e+02   6.87667250e+02\n",
      "   2.73965433e+04   8.38748125e+03   2.30126439e+04   1.85636413e+04\n",
      "   2.10342476e+03   4.16827225e+03   4.10798880e+03   1.01211997e+03\n",
      "   1.37675315e+04   1.32474219e+05   4.36462001e+03   2.53927925e+05\n",
      "   3.22775772e+04   3.36438851e+03   1.11384319e+03   1.36970800e+03\n",
      "   4.90033674e+04   5.72075605e+03   1.52211845e+03   1.44854791e+04\n",
      "   1.61643247e+05   2.69096469e+03   4.20956417e+04   8.39326658e+02\n",
      "   2.72050623e+03   4.80168700e+02   3.76841072e+03   4.82974550e+02\n",
      "   1.30735360e+04   1.68613986e+03   9.11838299e+03   1.76416756e+03\n",
      "   8.38160494e+04   1.56634489e+04   1.25876684e+03   2.06637442e+03\n",
      "   4.93451376e+02   8.55748548e+01   1.39853678e+03   7.60167542e+03\n",
      "   3.16595275e+03   2.18254493e+03   2.45231236e+03   1.25450459e+02\n",
      "   2.22723955e+05   5.26376008e+03   1.55922915e+04   9.46786120e+02\n",
      "   2.59249723e+04   6.59742239e+02   4.21783565e+05   1.18694596e+04\n",
      "   1.19208558e+03   9.17178580e+05   1.07121591e+03   1.08728676e+03\n",
      "   8.96104036e+02   3.72522326e+02   6.33674672e+03   1.93318636e+03\n",
      "   7.98339654e+05   1.04447528e+05   4.41697344e+03   2.25218050e+04\n",
      "   1.91936555e+03   5.25085782e+04   5.86704164e+04   2.51499489e+03\n",
      "   2.73034092e+03   1.51066142e+04   4.64718674e+05   3.25357849e+02\n",
      "   2.62812435e+03   1.45798514e+04   1.45639925e+03   6.70286704e+04\n",
      "   3.34224176e+02   1.69553359e+02   1.05683998e+04   4.17952738e+04\n",
      "   2.00231781e+03   1.11825568e+03   8.63529668e+02   9.35405309e+02\n",
      "   6.04981354e+04   2.80971267e+03   3.58209562e+03   4.13495081e+04\n",
      "   7.93161056e+03   1.75514706e+03   1.34415941e+04   5.89706434e+03\n",
      "   1.31505638e+05   1.03338702e+04   1.14134495e+04   6.42461733e+04\n",
      "   5.02532956e+03   6.29066000e+03   6.85447861e+03   2.73080337e+03\n",
      "   3.61207896e+04   6.14228555e+03   1.05037521e+03   1.19776734e+03\n",
      "   7.61226918e+03   7.81478021e+02   9.92589182e+04   3.06686876e+03\n",
      "   3.85378844e+04   5.82720199e+02   9.28512829e+03   1.03606432e+03\n",
      "   8.12662712e+02   3.24379997e+03   7.00710068e+04   5.41158833e+04\n",
      "   4.62397550e+03   3.30880287e+03   1.36106769e+03   5.80499927e+04\n",
      "   2.68033578e+03   5.97102778e+02   1.40376028e+03   1.22334313e+03\n",
      "   2.12514511e+03   2.55032938e+02   3.91043449e+04   2.15422364e+04\n",
      "   2.21850886e+04   2.57434136e+03   1.20428209e+03   6.15132366e+02\n",
      "   4.55339150e+04   1.99601505e+02   1.95153218e+04   2.37822388e+03\n",
      "   2.11276202e+03   9.68156314e+02   8.67563868e+03   9.25375879e+02\n",
      "   1.31121964e+03   1.80941933e+04   1.27414690e+05   1.81163261e+03\n",
      "   1.79180942e+04   2.45611243e+04   8.95420238e+02   1.40059579e+04\n",
      "   7.21960236e+04   1.08648826e+03   5.24762626e+03   1.61484330e+03\n",
      "   5.76349631e+05   3.09166655e+04   2.84040551e+03   2.75915510e+02\n",
      "   2.94115557e+03   1.31245785e+04   5.99212076e+04   1.20536581e+04\n",
      "   1.91112158e+04   5.21216893e+02   1.28437459e+04   7.38667629e+02\n",
      "   6.20061735e+02   1.90634058e+02   8.29910903e+05   2.50566966e+04\n",
      "   2.43905078e+03   1.42067698e+03   2.83958492e+03   1.48957479e+04\n",
      "   9.34643850e+03   5.52253976e+02   1.96020894e+03   1.98719360e+02\n",
      "   3.35360225e+03   1.31153133e+04   3.11569499e+03   2.05224366e+02\n",
      "   8.39424272e+02   7.22939224e+03   3.38075616e+04   1.22279410e+03\n",
      "   4.87680692e+02   1.33179828e+02   2.88264281e+03   1.05440000e+03\n",
      "   2.03110704e+04   4.89599524e+02   2.19295015e+03   3.14335612e+04\n",
      "   7.99873470e+03   1.12348400e+05   2.22994867e+06   2.65343527e+03\n",
      "   4.61643687e+05   1.32428424e+03   1.03610473e+04   2.73317467e+03\n",
      "   9.23531215e+03   7.88195267e+02   7.23317943e+04   3.43807628e+02\n",
      "   1.06903354e+03   1.34226172e+04   1.25989485e+04   5.09335605e+05\n",
      "   1.10633120e+03   8.51552113e+02   5.97866151e+02   5.03731262e+02\n",
      "   2.85291702e+04   2.97380957e+03   7.07149183e+02   2.18128638e+04\n",
      "   1.48839700e+04   5.18763780e+02   1.02024256e+05   2.78167430e+04\n",
      "   2.28498669e+04   1.12654162e+02   6.36424387e+02   1.28231773e+03\n",
      "   1.19059991e+04   2.91342856e+05   6.84601244e+03   1.97694436e+03\n",
      "   4.76484047e+03   5.26255387e+05   4.72709678e+02   3.82893446e+05\n",
      "   2.85137644e+04   1.15952626e+02   1.21964670e+04   4.29732175e+02\n",
      "   2.89981526e+04   3.82339609e+02   5.15236584e+03   5.05162869e+02\n",
      "   4.75123380e+04   2.99932717e+03   3.51431681e+02   9.39826618e+02\n",
      "   2.52488492e+04   1.21231231e+04   1.45674968e+04   3.66675785e+04\n",
      "   2.47907203e+03   1.67379427e+03   1.14813818e+04   1.56900592e+02\n",
      "   1.41932741e+04   1.33434217e+04   1.99608376e+03   1.91075638e+04\n",
      "   5.26867453e+03   1.75558631e+04   5.72310102e+02   1.78839425e+03\n",
      "   7.16938830e+03   2.17775755e+04   9.32573576e+04   7.25772901e+03\n",
      "   1.76610164e+03   9.40743284e+02   1.06259523e+04   9.06603193e+02\n",
      "   1.17230478e+03   3.15203320e+02   3.56302558e+03   8.83465611e+04\n",
      "   4.80595877e+03   6.93159439e+03   6.54022707e+02   6.99855854e+02\n",
      "   2.13338966e+04   2.31758676e+02   2.46258815e+03   2.30786447e+04\n",
      "   1.42703855e+03   6.82570488e+02   8.06247209e+04   9.09609303e+03\n",
      "   1.59661140e+04   1.32891554e+03   2.59141197e+05   9.78261261e+03\n",
      "   2.76523452e+04   9.88369228e+03   5.14879287e+02   2.45836128e+04\n",
      "   6.29189907e+03   4.31677126e+05   6.34946434e+03   5.80278525e+05\n",
      "   5.09710064e+03   2.49795686e+03   1.59951269e+03   1.59135741e+04\n",
      "   5.88650355e+02   3.99867443e+03   1.00224830e+03   8.30129733e+02\n",
      "   2.64028552e+03   1.98783676e+02   1.00729054e+05   7.62049660e+02\n",
      "   3.39464727e+03   9.84546271e+04   1.09985668e+03   2.90380364e+04\n",
      "   1.13658498e+04   4.61805534e+03   3.37210703e+03   1.13697343e+05\n",
      "   1.11684328e+04   3.39893502e+03   1.78029435e+04   1.75799121e+03\n",
      "   5.08970790e+02   1.83419051e+05   1.55573913e+04   1.71779285e+03\n",
      "   3.17843018e+04   3.09917910e+02   1.95271902e+04   6.27193191e+02\n",
      "   6.94315225e+03   2.54876141e+03   1.41495856e+04   4.41348558e+03\n",
      "   7.00500165e+04   1.10449979e+03   1.48953475e+03   1.33974500e+04\n",
      "   6.21758783e+03   4.57697959e+05   1.64157815e+04   4.78022747e+02\n",
      "   6.19931801e+02   5.02966702e+02   2.81031153e+03   5.93800634e+02\n",
      "   1.34901872e+05   2.12248627e+03   1.10644009e+03   1.90452653e+03\n",
      "   9.96813363e+03   2.08615234e+04   1.51852017e+03   5.11765204e+02\n",
      "   1.10689774e+03   3.72461486e+02   5.51409817e+02   6.21300579e+02\n",
      "   5.21991174e+03   8.73994812e+02   1.24534591e+05   7.65106962e+02\n",
      "   5.60425364e+02   7.43513430e+03   1.53914058e+03   1.04158275e+05\n",
      "   2.72809702e+04   2.45525848e+04   2.41430081e+03   5.83795717e+02\n",
      "   1.14216163e+03   5.92061885e+03   4.83579054e+04   6.68087923e+01\n",
      "   1.41698601e+03   3.62007674e+04   1.35628092e+04   4.07588988e+03\n",
      "   1.00117293e+03   3.24810602e+03   7.52176819e+02   1.65247427e+04\n",
      "   6.56786876e+03   4.66474128e+03   7.24491291e+03   3.60027911e+04\n",
      "   3.15362555e+03   2.26584281e+02   3.95138377e+03   5.21732532e+05\n",
      "   1.58572780e+03   8.59449125e+04   4.93240376e+04   2.05674802e+03\n",
      "   8.42293366e+03   6.95867805e+04   1.98783283e+03   1.21870114e+03\n",
      "   2.06080116e+04   3.02008595e+02   2.33297423e+03   7.84743279e+03\n",
      "   6.56778546e+04   2.65029888e+02   2.95178940e+04   3.65120984e+02\n",
      "   4.08722060e+02   2.50844917e+02   6.51383145e+04   5.66602927e+03\n",
      "   3.05105623e+03   6.65386161e+04   5.43608379e+04   7.23059217e+02\n",
      "   4.12088276e+04   1.04174032e+03   4.06600655e+02   1.92119248e+02\n",
      "   2.28230726e+04   2.07587394e+03   4.61650785e+03   3.46584152e+03\n",
      "   1.35127668e+03   2.33267520e+03   5.15209720e+03   1.64167480e+05\n",
      "   1.41206715e+04   1.25896388e+04   1.31253936e+04   5.96994963e+02\n",
      "   1.54050538e+03   8.54571989e+03   2.35755467e+03   8.53891460e+03\n",
      "   1.85909822e+05   6.39138289e+03   2.64250599e+04   2.85053782e+04\n",
      "   1.40154961e+05   2.13579721e+05   2.54738581e+03   2.68273612e+04\n",
      "   2.77334283e+04   3.88492553e+02   2.16024803e+04   3.16128042e+02\n",
      "   1.90175113e+04   3.41766301e+02   1.43642821e+04   1.80915209e+03\n",
      "   1.06871008e+05   2.47500574e+02   1.26817006e+04   3.35831527e+04\n",
      "   1.15924517e+04   6.04141531e+02   9.46596702e+03   2.73264156e+03\n",
      "   1.36420532e+05   3.92195705e+03   2.49483056e+03   5.73103802e+03\n",
      "   1.25102293e+04   2.39555228e+03   1.07226735e+04   9.74968611e+02\n",
      "   1.51890402e+04   8.27112491e+02   2.39387136e+04   9.80599602e+02\n",
      "   9.74220708e+04   3.26876911e+04   9.07111139e+03   2.64419314e+04\n",
      "   1.28613776e+04   3.16171211e+03   5.10790496e+03   1.74635965e+03\n",
      "   5.98146005e+04   3.55576192e+02   1.23638096e+03   2.90627723e+05\n",
      "   2.32285535e+05   3.52969679e+03   2.91212479e+04   8.30054064e+04\n",
      "   3.57496902e+04   6.46320777e+02   2.08627798e+05   1.32318712e+03\n",
      "   6.41769906e+04   1.61639375e+04   1.04070585e+04   1.57670419e+04\n",
      "   2.20821329e+04   2.40691979e+03   5.81760699e+03   3.60117374e+02\n",
      "   2.77434194e+04   6.77765757e+03   1.22630314e+04   3.34092503e+04\n",
      "   9.16939669e+04   1.22879869e+05   2.28636583e+04   6.84952939e+02\n",
      "   8.40681781e+04   1.05756167e+02   7.89814132e+04   3.19064246e+03\n",
      "   5.90423839e+03   1.58677047e+03   6.15074473e+05   6.77781415e+02\n",
      "   3.69089132e+05   8.46633959e+02   5.85571778e+02   7.24221136e+03\n",
      "   1.34702538e+04   1.03337611e+03   4.76925671e+04   3.17056585e+04\n",
      "   8.36453266e+03   1.45254285e+02   1.91580528e+04   3.50478843e+07\n",
      "   5.22212553e+04   5.53987988e+04   5.86301065e+04   2.57706026e+03\n",
      "   1.88914709e+04   8.45482469e+02   7.95565068e+04   2.04684581e+04\n",
      "   1.76356137e+03   7.32453723e+04   1.14198826e+04   2.20408201e+03\n",
      "   1.61645188e+05   3.23176364e+02   1.72874248e+04   4.10486335e+05\n",
      "   2.62519929e+03   5.07028577e+04   1.03923863e+03   4.50856866e+04\n",
      "   4.82123191e+04   1.38517500e+02   9.75326436e+02   4.11684948e+02\n",
      "   3.95313960e+05   6.10667936e+02   1.09170158e+03   3.56833765e+03\n",
      "   1.35619096e+04   2.67059354e+02   8.09680907e+04   3.62845619e+03\n",
      "   1.10426166e+04   3.39058457e+04   5.85395177e+04   1.56173660e+04\n",
      "   1.10906864e+04   3.16128817e+04   4.72777241e+03   7.96414974e+02\n",
      "   1.87093339e+04   1.37272774e+03   1.68029406e+04   1.03485936e+04\n",
      "   6.08418735e+03   1.34556046e+04   6.68483211e+03   7.54986093e+03\n",
      "   7.03095115e+03   1.65146458e+04   3.14565624e+03   5.15781287e+02\n",
      "   7.55776416e+02   3.35130225e+04   9.54030991e+03   1.91104205e+03\n",
      "   1.46869952e+04   2.65089869e+05   1.97698047e+04   8.29396454e+03\n",
      "   3.64336144e+04   3.88359044e+03   7.66283117e+03   2.43393442e+03\n",
      "   2.91102164e+03   3.64232288e+02   1.71823017e+04   4.81004716e+02\n",
      "   1.70803847e+05   3.92751647e+04   3.30789840e+03   3.08630320e+03\n",
      "   5.76931863e+04   2.14880886e+04   9.61326294e+04   1.22457246e+03\n",
      "   1.24244426e+03   1.76135947e+03   9.39486973e+02   4.15052352e+03\n",
      "   2.01011575e+04   2.60995763e+02   5.46666206e+03   1.50985917e+04\n",
      "   4.31391064e+04   4.37563418e+03   4.41449215e+03   1.57481776e+04\n",
      "   3.43835926e+04   1.27378999e+04   1.55059884e+04   7.12577667e+03\n",
      "   2.15982629e+05   1.89177446e+03   1.20939703e+04   5.17515031e+03\n",
      "   3.56126977e+03   4.15496386e+03   1.63935173e+04   8.06407330e+02\n",
      "   2.62986078e+04   9.29984866e+02   4.52051139e+05   4.91600597e+04\n",
      "   1.71666615e+04   2.37278783e+04   1.22381992e+03   2.38122533e+02\n",
      "   3.96124196e+04   8.74380121e+03   2.29831072e+04   3.74326288e+02\n",
      "   1.61261251e+04   7.54583951e+02   5.73095139e+02   6.48212949e+05\n",
      "   1.25530731e+05   2.18641196e+05   2.45103744e+05   9.43646264e+02\n",
      "   6.45975278e+03   7.16893411e+02   8.40298638e+02   5.95433123e+03\n",
      "   2.63597569e+04   6.85802778e+03   2.64510965e+04   7.49278731e+02\n",
      "   1.12342769e+05   3.92972188e+04   5.27226966e+04   1.43192449e+03\n",
      "   2.11168957e+04   3.69519727e+04   6.41980750e+02   5.42797383e+02\n",
      "   2.88021918e+05   2.66442064e+04   7.82679371e+05   8.58476176e+04\n",
      "   2.50123562e+03   2.89802515e+03   1.25592858e+03   2.51514425e+02\n",
      "   9.46519169e+03   9.66131084e+03   2.46951557e+04   5.25399403e+03\n",
      "   8.81659361e+02   5.71154189e+03   2.98528556e+05   2.78961474e+03\n",
      "   2.68599344e+05   4.46687064e+03   1.15737340e+04   1.73458610e+03\n",
      "   5.90345036e+04   7.07766024e+03   9.36033739e+02   1.16380435e+05\n",
      "   4.90509621e+04   6.61748634e+03   6.63717746e+02   2.21012102e+03\n",
      "   3.12263946e+03   2.21578579e+02   4.62095723e+04   3.02055257e+04\n",
      "   1.02640389e+05   1.31580637e+04   7.05837518e+04   1.73328931e+03\n",
      "   8.15196600e+03   2.92869649e+04   2.05652776e+04   1.91764991e+03\n",
      "   8.00491624e+02   1.13194820e+04   1.69173708e+03   8.59460570e+02\n",
      "   1.85109433e+03   2.09228227e+03   9.11281687e+02   4.83149536e+03\n",
      "   1.40446808e+02   1.10293033e+05   6.34859395e+03   4.77301529e+05\n",
      "   1.07776427e+03   1.44922145e+05   4.42758674e+02   1.17241252e+04\n",
      "   1.13719987e+05   7.95355896e+04   5.16570235e+02   8.09050060e+03\n",
      "   1.34021091e+04   1.65807301e+04   1.60671205e+03   8.50026447e+03\n",
      "   7.73635119e+03   1.90986183e+04   5.36144677e+03   4.12472719e+04\n",
      "   2.34516280e+02   1.30778624e+03   1.20167371e+04   2.57921937e+03\n",
      "   9.04651851e+03   1.36423906e+03   5.88081783e+03   5.63281805e+02\n",
      "   1.63964708e+04   4.43583239e+03   7.23693420e+02   5.05183538e+05\n",
      "   7.23057989e+03   1.94984027e+04   6.75186077e+04   8.40536780e+03\n",
      "   3.40937537e+03   1.44361561e+02   2.42839537e+04   1.96462514e+03\n",
      "   8.21990467e+04   1.07827672e+03   1.91862656e+04   6.49751864e+03\n",
      "   1.57130283e+03   8.62065938e+04   5.21268989e+03   9.47617678e+03\n",
      "   6.67831518e+04   1.85515476e+04   1.01676176e+04   1.31227127e+04\n",
      "   1.49451611e+02   7.65803122e+03   2.04602988e+04   1.97342561e+03\n",
      "   2.87950243e+04   2.80067446e+03   6.36170209e+03   4.62226233e+03]\n",
      "probabilities [[ 0.04975563  0.14199325  0.08223513 ...,  0.12145376  0.14583975\n",
      "   0.08757106]\n",
      " [ 0.21143307  0.15101869  0.10346944 ...,  0.14362548  0.11030357\n",
      "   0.10127918]\n",
      " [ 0.05890832  0.09174494  0.13699501 ...,  0.07278205  0.07406096\n",
      "   0.03467922]\n",
      " ..., \n",
      " [ 0.12194865  0.11494748  0.17831351 ...,  0.09235234  0.06611047\n",
      "   0.15570067]\n",
      " [ 0.08257615  0.08376801  0.07190298 ...,  0.07444322  0.10954209\n",
      "   0.08697669]\n",
      " [ 0.10163927  0.14438025  0.10552725 ...,  0.17042198  0.12222136\n",
      "   0.1926519 ]]\n"
     ]
    }
   ],
   "source": [
    "input_size = 784       # input vector size\n",
    "num_classes = 10       # number of classes\n",
    "lam = 0.0001           # weight decay parameter\n",
    "regressor = SoftmaxRegression(input_size, num_classes, lam)\n",
    "cost, gradient = regressor.softmax_cost(theta, X.T, y.T)\n",
    "\n",
    "# print 'cost', cost\n",
    "# print 'gradient', gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'training_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-0bc60d70491c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0mtraining_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'training_data' is not defined"
     ]
    }
   ],
   "source": [
    "print training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 60000)\n",
      "(60000, 1)\n"
     ]
    }
   ],
   "source": [
    "print training_data.shape\n",
    "print training_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_data = None\n",
    "training_labels = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0  51 159 253 159  50   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0  48 238 252 252 252 237   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0  54 227 253 252 239 233 252  57   6   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0  10  60 224 252 253 252 202  84 252\n",
      " 253 122   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0 163 252 252 252 253 252 252  96 189 253 167   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0  51 238 253 253 190 114 253 228\n",
      "  47  79 255 168   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0  48 238 252 252 179  12  75 121  21   0   0 253 243  50   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0  38 165 253 233 208  84   0   0\n",
      "   0   0   0   0 253 252 165   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   7 178 252 240  71  19  28   0   0   0   0   0   0 253 252 195   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0  57 252 252  63   0   0   0\n",
      "   0   0   0   0   0   0 253 252 195   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0 198 253 190   0   0   0   0   0   0   0   0   0   0 255 253\n",
      " 196   0   0   0   0   0   0   0   0   0   0   0  76 246 252 112   0   0\n",
      "   0   0   0   0   0   0   0   0 253 252 148   0   0   0   0   0   0   0\n",
      "   0   0   0   0  85 252 230  25   0   0   0   0   0   0   0   0   7 135\n",
      " 253 186  12   0   0   0   0   0   0   0   0   0   0   0  85 252 223   0\n",
      "   0   0   0   0   0   0   0   7 131 252 225  71   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0  85 252 145   0   0   0   0   0   0   0  48 165\n",
      " 252 173   0   0   0   0   0   0   0   0   0   0   0   0   0   0  86 253\n",
      " 225   0   0   0   0   0   0 114 238 253 162   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0  85 252 249 146  48  29  85 178 225 253\n",
      " 223 167  56   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "  85 252 252 252 229 215 252 252 252 196 130   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0  28 199 252 252 253 252 252 233\n",
      " 145   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0  25 128 252 253 252 141  37   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0]\n",
      "[[0 4 1 9 2 1 3 1 4 3 5 3 6 1 7 2 8 6 9 4 0 9 1 1 2 4 3 2 7 3 8 6 9 0 5 6 0\n",
      "  7 6 1 8 7 9 3 9 8 5 9 3 3 0 7 4 9 8 0 9 4 1 4 4 6 0 4 5 6 1 0 0 1 7 1 6 3\n",
      "  0 2 1 1 7 9 0 2 6 7 8 3 9 0 4 6 7 4 6 8 0 7 8 3 1 5 7 1 7 1 1 6 3 0 2 9 3\n",
      "  1 1 0 4 9 2 0 0 2 0 2 7 1 8 6 4 1 6 3 4 5 9 1 3 3 8 5 4 7 7 4 2 8 5 8 6 7\n",
      "  3 4 6 1 9 9 6 0 3 7 2 8 2 9 4 4 6 4 9 7 0 9 2 9 5 1 5 9 1 2 3 2 3 5 9 1 7\n",
      "  6 2 8 2 2 5 0 7 4 9 7 8 3 2 1 1 8 3 6 1 0 3 1 0 0 1 7 2 7 3 0 4 6 5 2 6 4\n",
      "  7 1 8 9 9 3 0 7 1 0 2 0 3 5 4 6 5 8 6 3 7 5 8 0 9 1 0 3 1 2 2 3 3 6 4 7 5\n",
      "  0 6 2 7 9 8 5 9 2 1 1 4 4 5 6 4 1 2 5 3 9 3 9 0 5 9 6 5 7 4 1 3 4 0 4 8 0\n",
      "  4 3 6 8 7 6 0 9 7 5 7 2 1 1 6 8 9 4 1 5 2 2 9 0 3 9 6 7 2 0 3 5 4 3 6 5 8\n",
      "  9 5 4 7 4 2 7 3 4 8 9 1 9 2 8 7 9 1 8 7 4 1 3 1 1 0 2 3 9 4 9 2 1 6 8 4 7\n",
      "  7 4 4 9 2 5 7 2 4 4 2 1 9 7 2 8 7 6 9 2 2 3 8 1 6 5 1 1 0 2 6 4 5 8 3 1 5\n",
      "  1 9 2 7 4 4 4 8 1 5 8 9 5 6 7 9 9 3 7 0 9 0 6 6 2 3 9 0 7 5 4 8 0 9 4 1 2\n",
      "  8 7 1 2 6 1 0 3 0 1 1 8 2 0 3 9 4 0 5 0 6 1 7 7 8 1 9 2 0 5 1 2 2 7 3 5 4\n",
      "  9 7 1 8 3 9 6 0 3 1 1 2 6 3 5 7 6 8 3 9 5 8 5 7 6 1 1 3 1 7 5 5 5 2 5 8 7\n",
      "  0 9 7 7 5 0 9 0 0 8 9 2 4 8 1 6 1 6 5 1 8 3 4 0 5 5 8 3 6 2 3 9 2 1 1 5 2\n",
      "  1 3 2 8 7 3 7 2 4 6 9 7 2 4 2 8 1 1 3 8 4 0 6 5 9 3 0 9 2 4 7 1 2 9 4 2 6\n",
      "  1 8 9 0 6 6 7 9 9 8 0 1 4 4 6 7 1 5 7 0 3 5 8 4 7 1 2 5 9 5 6 7 5 9 8 8 3\n",
      "  6 9 7 0 7 5 7 1 1 0 7 9 2 3 7 3 2 4 1 6 2 7 5 5 7 4 0 2 6 3 6 4 0 4 2 6 0\n",
      "  0 0 0 3 1 6 2 2 3 1 4 1 5 4 6 4 7 2 8 7 9 2 0 5 1 4 2 8 3 2 4 1 5 4 6 0 7\n",
      "  9 8 4 9 8 0 1 1 0 2 2 3 2 4 4 5 8 6 5 7 7 8 8 9 7 4 7 3 2 0 8 6 8 6 1 6 8\n",
      "  9 4 0 9 0 4 1 5 4 7 5 3 7 4 9 8 5 8 6 3 8 6 9 9 1 8 3 5 8 6 5 9 7 2 5 0 8\n",
      "  5 1 1 0 9 1 8 6 7 0 9 3 0 8 8 9 6 7 8 4 7 5 9 2 6 7 4 5 9 2 3 1 6 3 9 2 2\n",
      "  5 6 8 0 7 7 1 9 8 7 0 9 9 4 6 2 8 5 1 4 1 5 5 1 7 3 6 4 3 2 5 6 4 4 0 4 4\n",
      "  6 7 2 4 3 3 8 0 0 3 2 2 9 8 2 3 7 0 1 1 0 2 3 3 8 4 3 5 7 6 4 7 7 8 5 9 7\n",
      "  0 3 1 6 2 4 3 4 4 7 5 9 6 9 0 7 1 4 2 7 3 6 7 5 8 4 5 5 2 7 1 1 5 6 8 5 8\n",
      "  4 0 7 9 9 2 9 7 7 8 7 4 2 6 9 1 7 0 6 4 2 5 7 0 7 1 0 3 7 6 5 0 6 1 5 1 7\n",
      "  8 5 0 3 4 7 7 5 7 8 6 9 3 8 6 1 0 9 7 1 3 0 5 6 4 4 2 4 4 3 1 7 7 6 0 3 6\n",
      "  0]]\n",
      "[[  3.85660322e-04   1.03759747e-05   3.16824117e-04 ...,   3.80265356e-04\n",
      "    8.45554183e-05   4.41699071e-05]\n",
      " [  3.42679909e-04   4.76696673e-04   1.97413316e-06 ...,   1.45938034e-04\n",
      "    4.58887061e-04   3.57287892e-04]\n",
      " [  2.71272184e-04   7.10850238e-05   1.86670380e-04 ...,   2.56569121e-04\n",
      "    3.25198591e-04   3.00519477e-04]\n",
      " ..., \n",
      " [  3.48713905e-04   1.56844888e-04   1.02725541e-04 ...,   9.49769320e-05\n",
      "    3.15085083e-04   4.70179917e-04]\n",
      " [  4.94031613e-05   2.77312323e-04   1.47121887e-04 ...,   1.56144800e-04\n",
      "    4.21690755e-04   4.74061754e-04]\n",
      " [  4.46830018e-04   1.25052283e-04   4.76457935e-05 ...,   4.98374494e-04\n",
      "    1.15055453e-04   4.47175715e-04]]\n"
     ]
    }
   ],
   "source": [
    "print X[0]\n",
    "print y.T\n",
    "print theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "[[  3.85660322e-04   1.03759747e-05   3.16824117e-04 ...,   3.80265356e-04\n",
      "    8.45554183e-05   4.41699071e-05]\n",
      " [  3.42679909e-04   4.76696673e-04   1.97413316e-06 ...,   1.45938034e-04\n",
      "    4.58887061e-04   3.57287892e-04]\n",
      " [  2.71272184e-04   7.10850238e-05   1.86670380e-04 ...,   2.56569121e-04\n",
      "    3.25198591e-04   3.00519477e-04]\n",
      " ..., \n",
      " [  3.48713905e-04   1.56844888e-04   1.02725541e-04 ...,   9.49769320e-05\n",
      "    3.15085083e-04   4.70179917e-04]\n",
      " [  4.94031613e-05   2.77312323e-04   1.47121887e-04 ...,   1.56144800e-04\n",
      "    4.21690755e-04   4.74061754e-04]\n",
      " [  4.46830018e-04   1.25052283e-04   4.76457935e-05 ...,   4.98374494e-04\n",
      "    1.15055453e-04   4.47175715e-04]]\n"
     ]
    }
   ],
   "source": [
    "print X\n",
    "# print y\n",
    "print theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7.02750991  7.65829303  8.22492154 ...,  7.57622251  7.71207028\n",
      "   7.86756045]\n",
      " [ 4.59951021  5.17384386  4.93871541 ...,  4.73542002  4.88209247\n",
      "   5.10557152]\n",
      " [ 3.966941    4.10703155  4.34924595 ...,  3.72894257  4.59960936\n",
      "   4.79950616]\n",
      " ..., \n",
      " [ 4.94369233  5.86620593  5.6433769  ...,  4.93204326  5.88177398\n",
      "   5.57196998]\n",
      " [ 6.09087626  6.60208114  7.0593024  ...,  5.8579878   6.36091286\n",
      "   6.73509632]\n",
      " [ 5.75268281  6.29468164  6.62473448 ...,  5.72080664  5.6044771\n",
      "   6.28099406]]\n"
     ]
    }
   ],
   "source": [
    "print np.dot(X, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 10)\n",
      "[[  3.85660322e-04   1.03759747e-05   3.16824117e-04 ...,   4.66530193e-04\n",
      "    2.79538175e-04   3.08552539e-04]\n",
      " [  2.34494602e-04   4.97414883e-04   4.66736855e-04 ...,   3.08559443e-04\n",
      "    2.61600180e-04   1.31760154e-04]\n",
      " [  2.66439515e-04   1.67756526e-04   1.53673573e-04 ...,   5.97942741e-05\n",
      "    1.59763830e-04   2.88624047e-04]\n",
      " ..., \n",
      " [  4.30791245e-04   3.23250704e-04   1.18635170e-04 ...,   4.50958641e-04\n",
      "    2.83464425e-04   1.39533453e-04]\n",
      " [  2.12040461e-04   6.69079080e-05   3.39008463e-04 ...,   4.56860688e-04\n",
      "    3.74694615e-04   2.45380228e-04]\n",
      " [  3.84250382e-04   1.95418337e-05   1.75991613e-04 ...,   4.98374494e-04\n",
      "    1.15055453e-04   4.47175715e-04]]\n",
      "[[  3.85660322e-04   3.42679909e-04   2.71272184e-04 ...,   3.48713905e-04\n",
      "    4.94031613e-05   4.46830018e-04]\n",
      " [  1.03759747e-05   4.76696673e-04   7.10850238e-05 ...,   1.56844888e-04\n",
      "    2.77312323e-04   1.25052283e-04]\n",
      " [  3.16824117e-04   1.97413316e-06   1.86670380e-04 ...,   1.02725541e-04\n",
      "    1.47121887e-04   4.76457935e-05]\n",
      " ..., \n",
      " [  3.80265356e-04   1.45938034e-04   2.56569121e-04 ...,   9.49769320e-05\n",
      "    1.56144800e-04   4.98374494e-04]\n",
      " [  8.45554183e-05   4.58887061e-04   3.25198591e-04 ...,   3.15085083e-04\n",
      "    4.21690755e-04   1.15055453e-04]\n",
      " [  4.41699071e-05   3.57287892e-04   3.00519477e-04 ...,   4.70179917e-04\n",
      "    4.74061754e-04   4.47175715e-04]]\n"
     ]
    }
   ],
   "source": [
    "print theta.shape\n",
    "theta_reshaped = theta.reshape(num_classes, input_size)\n",
    "# print theta_reshaped.shape\n",
    "# np.dot(theta_reshaped, X.T)\n",
    "print theta_reshaped\n",
    "print theta.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
